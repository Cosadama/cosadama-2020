{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🦉COSADAMA Introduction to Data Science Study\n",
    "\n",
    "- 일자: 2020-04-06\n",
    "- 작성자: 박하람 \n",
    "- 참고자료: 파이썬으로 데이터 주무르기(민형기), 점프 투 파이썬(박응용), 파이썬 입문과 크롤링 기초 부트캠프(잔재미코딩)\n",
    "- 교재: 125 - 138쪽 (깃헙 교안으로 공부하면서, 궁금한 부분은 교재를 참고하세요. 혹은 깃헙 교안을 다 보고 교재를 복습 겸 읽어봐도 됩니다!)\n",
    "\n",
    "## 4. 웹 크롤링과 정규 표현식\n",
    "\n",
    "이번 주차에는 드디어 재밌는 **웹 크롤링**과 외계어 같은 **정규 표현식**에 대해 알아보려고 합니다. 웹 크롤링이라는 것은 웹에서 내가 원하는 데이터를 긁어오는 것을 말하는데요, 웹 크롤링을 하면 뭔가 내가 프로그래밍을 배웠긴 배웠구나라는 신기한 생각이 든답니다😀 웹 크롤링도 엄청나게 방대한 기술이 있기 때문에 기초적인 수준을 먼저 다뤄보려 합니다. \n",
    "\n",
    "이번 주에 사용할 모듈은 **BeatifulSoup**과 **urllib 라이브러리**를 활용해 한국 일보 사회 파트의 타이틀만 모아 볼 것이구요, 엑셀을 열고 기록하는데 사용되는 모듈 **openpyxl**을 사용해 가져온 타이틀들을 엑셀 파일에 저장하고 읽어도 볼 겁니다. 웹에서 정보를 가져오기 위해서는 **HTML/CSS**에 대한 기본지식이 있어야 하니 이 또한 웹 크롤링에 필요한 부분만(아주 작은 부분) 뽑아서 학습해 볼 겁니다. 그 후에는 **정규표현식**을 배워 좀 더 편하게 원하는 데이터만 가져올 수 있도록 할 것이구요! 이럴려고 한 것은 아니지만,,, (죄송하게도) 이번 주차는 학습 분량이 꽤 많습니다...!😂 정말 줄이고 줄여서 담은 내용이라 하나도 뺄게 없답니다..ㅎㅎ 이렇게 열심히 좋은 자료를 찾아서 만든 저를 위해서라도 열공해주세요🥰\n",
    "\n",
    "### 4.1 HTML/CSS 기초 \n",
    "\n",
    "웹 크롤링을 하려면 웹이 어떤 문법으로 만들어지는지부터 우선 알아야 합니다. html는 웹페이지를 만들기 위한 언어로 웹브라우저 위에서 동작하는 언어에요. CSS는 간단히 웹 브라우저를 꾸며주는 언어라고 생각하면 되구요. 웹이 어떻게 구성되어 있는지를 이해해야 웹 크롤링도 할 수 있어요. 아래 링크를 보고 해당 부분만 학습하고 오면 됩니다! Code Academy의 HTML/CSS 기초는 무료 강의에다가, 직접 코드를 작성해볼 수 있는 에디터도 있어 직관적으로 배울 수 있어요! 다만 영어라는 점...! (영어에 익숙해지는 것이 좋습니다..ㅜㅜ 안타깝게도 좋은 자료들은 영어로 다 적혀져 있더라구요.) \n",
    "\n",
    "- [Introduction to HTML](https://www.codecademy.com/learn/learn-html) - syllabus 1. elements & structured의 Introduction to HTML과 HTML Document Standard 학습하기\n",
    "- [Learn CSS](https://www.codecademy.com/learn/learn-css) - syllabus 1. Selectors and Visual Rules의 CSS Setup & Selectors 학습하기\n",
    "\n",
    "처음 배우시면 이것을 끝내시는데 하루(3시간 이상) 정도는 걸릴 것 같아요. html/css를 아시는 분들은 이 파트를 넘어가시면 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 웹 데이터를 가져오는 BeautifulSoup \n",
    "\n",
    "- 구글 드라이브의 **02. test_first.html** 다운받기 \n",
    "- [BeautifulSoup4 공식문서](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) - 한글 번역 있음\n",
    "\n",
    "**BeautifulSoup**은 인터넷에서 웹 페이지의 내용을 가져오는 모듈이에요. 웹 크롤링할 때 거의 필수적으로 사용하는 것 같아요. 드라이브에 있는 html 파일을 다운 받으시고 학습하면 됩니다. 더 궁금한 문법이 있다면 위의 공식문서에 한글 번역이 있으니 찬찬히 읽어보는 것도 좋은 방법일 것 같습니다! \n",
    "\n",
    "우선 beautifulsoup을 import 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 일단 저장된 html 파일을 읽어오는 것이기 때문에 아래와 같은 방식으로 써줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html>\n",
      "<head>\n",
      "<title>Very Simple HTML Code by PinkWink</title>\n",
      "</head>\n",
      "<body>\n",
      "<div>\n",
      "<p class=\"inner-text first-item\" id=\"first\">\n",
      "                Happy PinkWink.\n",
      "                <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
      "</p>\n",
      "<p class=\"inner-text second-item\">\n",
      "                Happy Data Science.\n",
      "                <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
      "</p>\n",
      "</div>\n",
      "<p class=\"outer-text first-item\" id=\"second\">\n",
      "<b>\n",
      "                Data Science is funny.\n",
      "            </b>\n",
      "</p>\n",
      "<p class=\"outer-text\">\n",
      "<b>\n",
      "                All I need is Love.\n",
      "            </b>\n",
      "</p>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "page = open(\"03. test_first.html\", \"r\").read()\n",
    "soup = BeautifulSoup(page, 'html.parser') # Python’s html.parser - 문서 전체를 저장한 변수 \n",
    "print(soup) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup 객체를 생성해주고, soup을 프린트했더니 html의 구조가 나오죠? 더 이쁘게 보여주려면 아래와 같이 하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Very Simple HTML Code by PinkWink\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <div>\n",
      "   <p class=\"inner-text first-item\" id=\"first\">\n",
      "    Happy PinkWink.\n",
      "    <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">\n",
      "     PinkWink\n",
      "    </a>\n",
      "   </p>\n",
      "   <p class=\"inner-text second-item\">\n",
      "    Happy Data Science.\n",
      "    <a href=\"https://www.python.org\" id=\"py-link\">\n",
      "     Python\n",
      "    </a>\n",
      "   </p>\n",
      "  </div>\n",
      "  <p class=\"outer-text first-item\" id=\"second\">\n",
      "   <b>\n",
      "    Data Science is funny.\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"outer-text\">\n",
      "   <b>\n",
      "    All I need is Love.\n",
      "   </b>\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())  # 태그를 구분하기 쉽게 해주는 명령"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 잘 구조화해서 보여주죠? VSC 에디터에는 프리티어라는 확장 프로그램이 있어서 이렇게 구조화시켜주면서 색깔 다르게 변화시켜주는데 정말 아주 예뻐요..ㅎㅎ (급 VSC 자랑.. 개발할 때는 주로 이것을 사용합니다.)\n",
    "\n",
    "soup에 담긴 것을 리스트 형태로 바꿔주어야 인덱싱할 수 있어요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['html', '\\n', <html>\n",
       " <head>\n",
       " <title>Very Simple HTML Code by PinkWink</title>\n",
       " </head>\n",
       " <body>\n",
       " <div>\n",
       " <p class=\"inner-text first-item\" id=\"first\">\n",
       "                 Happy PinkWink.\n",
       "                 <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       " </p>\n",
       " <p class=\"inner-text second-item\">\n",
       "                 Happy Data Science.\n",
       "                 <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       " </p>\n",
       " </div>\n",
       " <p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>\n",
       " <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>\n",
       " </body>\n",
       " </html>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(soup.children) # soup에서 한 단계 아래에서 포함된 태그들을 알고 싶으면 children -> list로 묶어라 (인덱싱 위해)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "리스트 표시가 보이죠? 여기서 html 변수를 [2]번째부터라 선언해 볼게요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head>\n",
       "<title>Very Simple HTML Code by PinkWink</title>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "<p class=\"inner-text first-item\" id=\"first\">\n",
       "                Happy PinkWink.\n",
       "                <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       "</p>\n",
       "<p class=\"inner-text second-item\">\n",
       "                Happy Data Science.\n",
       "                <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       "</p>\n",
       "</div>\n",
       "<p class=\"outer-text first-item\" id=\"second\">\n",
       "<b>\n",
       "                Data Science is funny.\n",
       "            </b>\n",
       "</p>\n",
       "<p class=\"outer-text\">\n",
       "<b>\n",
       "                All I need is Love.\n",
       "            </b>\n",
       "</p>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = list(soup.children)[2]\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "html 아래에 있는 자식을 가져오고 싶다면, .children을 사용하면 됩니다. 다만 줄바꿈 문자가 있기 때문에 같이 나와요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', <head>\n",
       " <title>Very Simple HTML Code by PinkWink</title>\n",
       " </head>, '\\n', <body>\n",
       " <div>\n",
       " <p class=\"inner-text first-item\" id=\"first\">\n",
       "                 Happy PinkWink.\n",
       "                 <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       " </p>\n",
       " <p class=\"inner-text second-item\">\n",
       "                 Happy Data Science.\n",
       "                 <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       " </p>\n",
       " </div>\n",
       " <p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>\n",
       " <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>\n",
       " </body>, '\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(html.children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "바디만 따로 변수에 저장해두고 싶으면 아래와 같이 하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<div>\n",
       "<p class=\"inner-text first-item\" id=\"first\">\n",
       "                Happy PinkWink.\n",
       "                <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       "</p>\n",
       "<p class=\"inner-text second-item\">\n",
       "                Happy Data Science.\n",
       "                <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       "</p>\n",
       "</div>\n",
       "<p class=\"outer-text first-item\" id=\"second\">\n",
       "<b>\n",
       "                Data Science is funny.\n",
       "            </b>\n",
       "</p>\n",
       "<p class=\"outer-text\">\n",
       "<b>\n",
       "                All I need is Love.\n",
       "            </b>\n",
       "</p>\n",
       "</body>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = list(html.children)[3]\n",
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게도 body를 보여줄 수 있어요! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<div>\n",
       "<p class=\"inner-text first-item\" id=\"first\">\n",
       "                Happy PinkWink.\n",
       "                <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       "</p>\n",
       "<p class=\"inner-text second-item\">\n",
       "                Happy Data Science.\n",
       "                <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       "</p>\n",
       "</div>\n",
       "<p class=\"outer-text first-item\" id=\"second\">\n",
       "<b>\n",
       "                Data Science is funny.\n",
       "            </b>\n",
       "</p>\n",
       "<p class=\"outer-text\">\n",
       "<b>\n",
       "                All I need is Love.\n",
       "            </b>\n",
       "</p>\n",
       "</body>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.body # 바로 body 보여줘라"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음은 실제 크롤링할 때 정말 유용하게 쓰이는 함수인데요, 바로 **find_all()**입니다. 이것은 괄호 안에 적힌 태그를 모두 가져와주는 기능을 수행해요. 반면에 **find()**는 제일 처음 태그 하나만 가져온답니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"inner-text first-item\" id=\"first\">\n",
       "                 Happy PinkWink.\n",
       "                 <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       " </p>, <p class=\"inner-text second-item\">\n",
       "                 Happy Data Science.\n",
       "                 <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       " </p>, <p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>, <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p')   # p 태그를 모두 찾아랏!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>, <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p', class_='outer-text')  # p 중 class가 outer-text인 태그만 모두 가져와라 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSS에서 id와 class를 배웠죠? class 명으로 태그를 가져오는 것도 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>, <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(class_='outer-text') # 이렇게 찾는 것도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"inner-text first-item\" id=\"first\">\n",
       "                 Happy PinkWink.\n",
       "                 <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       " </p>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(id=\"first\")   #id가 first인 것을 찾아라 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p class=\"inner-text first-item\" id=\"first\">\n",
       "                Happy PinkWink.\n",
       "                <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       "</p>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p') # p 제일 앞의 것을 찾아라"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같이 head만 가져올 수 있는데요, .next_sibling을 사용해서 그 다음의 태그를 가져올 수 있어요. 두번 쓸 수도 있고요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<head>\n",
       "<title>Very Simple HTML Code by PinkWink</title>\n",
       "</head>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.head.next_sibling # 그 다음 태그 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<div>\n",
       "<p class=\"inner-text first-item\" id=\"first\">\n",
       "                Happy PinkWink.\n",
       "                <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       "</p>\n",
       "<p class=\"inner-text second-item\">\n",
       "                Happy Data Science.\n",
       "                <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       "</p>\n",
       "</div>\n",
       "<p class=\"outer-text first-item\" id=\"second\">\n",
       "<b>\n",
       "                Data Science is funny.\n",
       "            </b>\n",
       "</p>\n",
       "<p class=\"outer-text\">\n",
       "<b>\n",
       "                All I need is Love.\n",
       "            </b>\n",
       "</p>\n",
       "</body>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.head.next_sibling.next_sibling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제는 html의 텍스트 정보들을 추출하는 방법에 대해 알아볼게요. 우선 p 태그를 모두 가져와 볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"inner-text first-item\" id=\"first\">\n",
       "                 Happy PinkWink.\n",
       "                 <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       " </p>, <p class=\"inner-text second-item\">\n",
       "                 Happy Data Science.\n",
       "                 <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       " </p>, <p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>, <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 find_all 함수를 쓰게 되면 리스트 형태로 도출이 되요. 그런데 저는 안에 있는 Happy PinkWink.,  Happy Data Science., Data Science is funny.,All I need is Love.라는 텍스트들만 가져오고 싶다면 어떻게 해야 할까요? 그 때에는 **.get_text()**를 쓰면 된답니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Happy PinkWink.\n",
      "                PinkWink\n",
      "\n",
      "\n",
      "                Happy Data Science.\n",
      "                Python\n",
      "\n",
      "\n",
      "\n",
      "                Data Science is funny.\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "                All I need is Love.\n",
      "            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each_tag in soup.find_all('p'): #리스트이기에 가능 \n",
    "    print(each_tag.get_text())  # 태그 안의 텍스트만 가져와라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n                Happy PinkWink.\\n                PinkWink\\n\\n\\n                Happy Data Science.\\n                Python\\n\\n\\n\\n\\n                Data Science is funny.\\n            \\n\\n\\n\\n                All I need is Love.\\n            \\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body.get_text()  # 태그가 있는 자리는 줄바꿈이 되고 전체 텍스트를 보여줌 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 anchor를 가져와 볼게요. 여기에 있는 링크들만 따로 뽑으려면 어떻게 해야 할까요? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>,\n",
       " <a href=\"https://www.python.org\" id=\"py-link\">Python</a>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = soup.find_all('a')\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역시나 리스트에 담겼으니 for문을 활용해 꺼내주겠다는게 감이 오죠? .get_text()로도 가능하지만 **.string**으로도 텍스트를 가져오는 것이 가능해요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PinkWink -> http://www.pinkwink.kr\n",
      "Python -> https://www.python.org\n"
     ]
    }
   ],
   "source": [
    "for each in links: \n",
    "    href = each['href'] #href만 가져와라 \n",
    "    text = each.string\n",
    "    print(text + ' -> ' + href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 urllib 라이브러리를 이용해 웹 크롤링하기 \n",
    "\n",
    "앞서 배운 BeautifulSoup 모듈과 urllib 라이브러리를 활용해 실제 사이트에서 웹을 크롤링해봅시다. 네이버 증권 국내증시에서 오늘의 코스피를 가져와 볼게요. \n",
    "\n",
    "우선 크롬의 개발자 도구를 활용하면 무척 쉽게 내가 가져오고자 하는 텍스트의 태그와 아이디, 클라스를 알 수 있어요. 교재 136-138쪽을 보시면 아실 수 있습니다. 혹은 인터넷을 사용해도 되구요. \n",
    "\n",
    "- [urllib 라이브러리](https://docs.python.org/3/library/urllib.request.html#module-urllib.request)\n",
    "- [네이버 증권 국내증시](https://finance.naver.com/sise/)\n",
    "\n",
    "urllib과 BeautifulSoup을 사용하는 방법은 아래와 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"num\" id=\"KOSPI_now\">1,609.97</span>, <span class=\"num\" id=\"KOSDAQ_now\">480.40</span>, <span class=\"num\" id=\"KPI200_now\">220.34</span>]\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = urlopen('https://finance.naver.com/sise/') # url 넣기\n",
    "soup = BeautifulSoup(res, \"html.parser\") # 객체 생성\n",
    "\n",
    "data = soup.find_all('span', class_='num') # 원하는 태크를 이용해 찾기\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "span 태그에 class가 num인게 여러개이죠? 이 중에서 저는 KOSPI만 알고 싶으니까 제일 첫번째 것만 가져와야 겠지요. 그럴 때는 리스트니 인덱싱을 쓰면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"num\" id=\"KOSPI_now\">1,609.97</span>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kospi = data[0]\n",
    "kospi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,609.97'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kospi.get_text()  # 문자만 가져와라 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 간단하게! 코스피 지수가 1,609.97이라는 것을 가져올 수 있었네요! \n",
    "\n",
    "### 4.4 뉴스 기사 타이틀 크롤링 \n",
    "\n",
    "이번에는 뉴스 기사 타이틀 크롤링하는 것에 대해 알아보려고 해요. 한국일보의 사회 면의 1페이지부터 5페이지까지의 뉴스 타이틀만 가져와볼겁니다.\n",
    "\n",
    "- [한국일보 사회](https://www.hankookilbo.com/News/Society/HC01)\n",
    "\n",
    "우선 필요한 모듈들을 임포트 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.hankookilbo.com/News/Society/HC01'\n",
    "page = urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "허거것, 내용이 무척 많죠? 이 중에서 크롬의 개발자 도구를 사용해 제가 찾고자 했던 타이틀이 어떤 태그와 클라스를 가지고 있는지 알아야 해요. 제가 크롬 개발자 도구를 이용해 보니 **title은 'p' 태그이고, class는 'title'**이더라구요. 그래서 이렇게 한번 가져와보도록 할게요. 내용이 너무 많으니 이렇게 하나씩 찾아가는 수밖에 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"title\">개학하면 코로나 위기 끝?  “재확산 우려… 생활방역이 대안”</p>,\n",
       " <p class=\"title\">[단독] ‘n번방’의 시작, 온라인 그루밍도 처벌한다</p>,\n",
       " <p class=\"title\">제주서 신종 코로나 확진자 2명 발생…스페인서 40여일 머물러</p>,\n",
       " <p class=\"title\">[속보]‘미국 발’ 코로나 19 확산 하나… 울산서 귀국 미국 유학생 확진</p>,\n",
       " <p class=\"title\">‘박사방’ 조주빈, 보육원 등 5개 시설서 231시간 봉사</p>,\n",
       " <p class=\"title\">[단독] “박사방 조주빈 협박에  피해여성·회원 극단적 선택까지”</p>,\n",
       " <p class=\"title\">‘미국서 귀국한 뒤 후각 감퇴’… 공항 검역 통과 ‘해외 역유입’ 어쩌나</p>,\n",
       " <p class=\"title\">25일 신상공개 조주빈, 검찰 포토라인에는 안 설 듯</p>,\n",
       " <p class=\"title\">[르포] 절망 빠진 파주 안보관광, “6개월째 개점휴업ㆍ하루 매출 5만원뿐”</p>,\n",
       " <p class=\"title\">‘제2의 n번방’ 갓갓 모방한 비밀 대화방 있었다</p>,\n",
       " <p class=\"title\">유럽서 입국 무증상 한국인 ‘자가격리 후 검사’로 전환</p>,\n",
       " <p class=\"title\">목포서 붕어빵 판매상 부부 코로나 확진… 전남 8명으로 늘어</p>,\n",
       " <p class=\"title\">소라넷이 시초, 솜방망이 처벌이 만든 디지털 성범죄</p>,\n",
       " <p class=\"title\">이재명 “모든 경기도민에 재난기본소득 10만원씩”</p>,\n",
       " <p class=\"title\">보수 커뮤니티, 봉사, 성착취범  조주빈의 비열한 행보</p>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = soup.find_all('p', class_='title')\n",
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제가 의도한대로 모두 잘 나왔네요! 원래는 이게 원하는 내용을 찾기까지 시간이 꽤 걸려요ㅎㅎ 그러니 항상 원래 이렇게 쉽다 라고 생각하지 마시길..! 저는 이 타이틀을 저장해두고 싶어서 빈 리스트를 만들고, 텍스트만 뽑아내 거기에 타이틀어 넣어주었답니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['개학하면 코로나 위기 끝?  “재확산 우려… 생활방역이 대안”', '[단독] ‘n번방’의 시작, 온라인 그루밍도 처벌한다', '제주서 신종 코로나 확진자 2명 발생…스페인서 40여일 머물러', '[속보]‘미국 발’ 코로나 19 확산 하나… 울산서 귀국 미국 유학생 확진', '‘박사방’ 조주빈, 보육원 등 5개 시설서 231시간 봉사', '[단독] “박사방 조주빈 협박에  피해여성·회원 극단적 선택까지”', '‘미국서 귀국한 뒤 후각 감퇴’… 공항 검역 통과 ‘해외 역유입’ 어쩌나', '25일 신상공개 조주빈, 검찰 포토라인에는 안 설 듯', '[르포] 절망 빠진 파주 안보관광, “6개월째 개점휴업ㆍ하루 매출 5만원뿐”', '‘제2의 n번방’ 갓갓 모방한 비밀 대화방 있었다', '유럽서 입국 무증상 한국인 ‘자가격리 후 검사’로 전환', '목포서 붕어빵 판매상 부부 코로나 확진… 전남 8명으로 늘어', '소라넷이 시초, 솜방망이 처벌이 만든 디지털 성범죄', '이재명 “모든 경기도민에 재난기본소득 10만원씩”', '보수 커뮤니티, 봉사, 성착취범  조주빈의 비열한 행보']\n"
     ]
    }
   ],
   "source": [
    "title_list = []\n",
    "\n",
    "for title in titles: \n",
    "    title_list.append(title.get_text())\n",
    "    \n",
    "print(title_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좀 더 이쁘게 볼까요? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개학하면 코로나 위기 끝?  “재확산 우려… 생활방역이 대안”\n",
      "[단독] ‘n번방’의 시작, 온라인 그루밍도 처벌한다\n",
      "제주서 신종 코로나 확진자 2명 발생…스페인서 40여일 머물러\n",
      "[속보]‘미국 발’ 코로나 19 확산 하나… 울산서 귀국 미국 유학생 확진\n",
      "‘박사방’ 조주빈, 보육원 등 5개 시설서 231시간 봉사\n",
      "[단독] “박사방 조주빈 협박에  피해여성·회원 극단적 선택까지”\n",
      "‘미국서 귀국한 뒤 후각 감퇴’… 공항 검역 통과 ‘해외 역유입’ 어쩌나\n",
      "25일 신상공개 조주빈, 검찰 포토라인에는 안 설 듯\n",
      "[르포] 절망 빠진 파주 안보관광, “6개월째 개점휴업ㆍ하루 매출 5만원뿐”\n",
      "‘제2의 n번방’ 갓갓 모방한 비밀 대화방 있었다\n",
      "유럽서 입국 무증상 한국인 ‘자가격리 후 검사’로 전환\n",
      "목포서 붕어빵 판매상 부부 코로나 확진… 전남 8명으로 늘어\n",
      "소라넷이 시초, 솜방망이 처벌이 만든 디지털 성범죄\n",
      "이재명 “모든 경기도민에 재난기본소득 10만원씩”\n",
      "보수 커뮤니티, 봉사, 성착취범  조주빈의 비열한 행보\n"
     ]
    }
   ],
   "source": [
    "for title in title_list: \n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 openpyxl 모듈로 웹 크롤링 자료 저장, 읽기 \n",
    "\n",
    "크롤링 했다면 그 결과를 담아야 나중에도 또 쓸 수 있겠죠? 후에 판다스에 연결해서 엑셀을 읽어낼 수도 있구요. 이렇게 엑셀에 담기 위해 필요한 모듈은 openpyxl이라는 것인데요, 아래 코드를 한번만 이해한다면 계속해서 활용하실 수 있을 거에요. \n",
    "\n",
    "#### 엑셀 파일에 자료 저장하기 \n",
    "\n",
    "아래의 코드를 한번 읽어보세요. 문법을 몰라도 대충 이게 이런 문법이구나 파악하실 수 있을 거에요. 정 모르겠다면 구글링으로 검색~ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 데이터로 엑셀파일 만들기 \n",
    "\n",
    "import openpyxl\n",
    "\n",
    "excel_file = openpyxl.Workbook()\n",
    "excel_file.remove(excel_file.active)\n",
    "excel_sheet = excel_file.create_sheet('안녕 시트')  # sheet 이름 작성 \n",
    "\n",
    "excel_sheet.column_dimensions['B'].width = 150 # column B 크기 정하기 \n",
    "\n",
    "for index in range(9):\n",
    "    excel_sheet.append([index, '안녕']) # 엑셀파일에서 이게 어떻게 구현됐을까요? \n",
    "\n",
    "cell_A1 = excel_sheet['A1']\n",
    "cell_A1.alignment = openpyxl.styles.Alignment(horizontal=\"center\")\n",
    "\n",
    "excel_file.save('test.xlsx')  # 엑셀 파일 이름 설정\n",
    "excel_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일이 저장된 폴더에 가보세요. 그렇다면 test.xlsx 파일이 만들어져 있을 것이랍니다! \n",
    "\n",
    "우리 앞에서 한국일보 타이틀을 추출한 것을 엑셀에다 담아볼까요? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen \n",
    "from bs4 import BeautifulSoup\n",
    "import openpyxl\n",
    "\n",
    "excel_file = openpyxl.Workbook()\n",
    "excel_file.remove(excel_file.active)\n",
    "excel_sheet = excel_file.create_sheet('사회뉴스')  # sheet 이름 작성 \n",
    "excel_sheet.column_dimensions['B'].width = 100   # 컬럼 크기 정하기 \n",
    "\n",
    "excel_sheet.append(['번호','제목']) #sheet에 표제목 넣기 \n",
    " \n",
    "url = 'https://www.hankookilbo.com/News/Society/HC01?Page=1'\n",
    "page = urlopen(url)\n",
    "soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "num = 0\n",
    "titles = soup.find_all('p', class_='title')   # 각 타이틀에서 p 중 class가 title인 것 가져오기 \n",
    "for title in titles: \n",
    "    # print(title.get_text())\n",
    "    num += 1\n",
    "    excel_sheet.append([num, title.get_text()])  # 타이틀 개수와 타이틀 내용 \n",
    "\n",
    "cell_A1 = excel_sheet['A1']\n",
    "cell_A1.alignment = openpyxl.styles.Alignment(horizontal=\"center\")  # A1 양식 center로!\n",
    "cell_B1 = excel_sheet['B1']\n",
    "cell_B1.alignment = openpyxl.styles.Alignment(horizontal=\"center\")  # B1 양식 center로!\n",
    "\n",
    "excel_file.save('한국일보 사회뉴스 타이틀 크롤링.xlsx')\n",
    "excel_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "요렇게 엑셀에 까지 담아보았습니다! 신기하죠? \n",
    "\n",
    "#### 엑셀 파일 자료 읽기 \n",
    "\n",
    "그럼 엑셀 파일을 읽을 수 있는 방법도 알아봐야겠죠?  이것도 아래와 같이 하면 됩니다. 코드를 생각해보세요. 쉽게 이해할 수 있어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번호 제목\n",
      "1 개학하면 코로나 위기 끝?  “재확산 우려… 생활방역이 대안”\n",
      "2 [단독] ‘n번방’의 시작, 온라인 그루밍도 처벌한다\n",
      "3 제주서 신종 코로나 확진자 2명 발생…스페인서 40여일 머물러\n",
      "4 [속보]‘미국 발’ 코로나 19 확산 하나… 울산서 귀국 미국 유학생 확진\n",
      "5 ‘박사방’ 조주빈, 보육원 등 5개 시설서 231시간 봉사\n",
      "6 [단독] “박사방 조주빈 협박에  피해여성·회원 극단적 선택까지”\n",
      "7 ‘미국서 귀국한 뒤 후각 감퇴’… 공항 검역 통과 ‘해외 역유입’ 어쩌나\n",
      "8 25일 신상공개 조주빈, 검찰 포토라인에는 안 설 듯\n",
      "9 [르포] 절망 빠진 파주 안보관광, “6개월째 개점휴업ㆍ하루 매출 5만원뿐”\n",
      "10 ‘제2의 n번방’ 갓갓 모방한 비밀 대화방 있었다\n",
      "11 유럽서 입국 무증상 한국인 ‘자가격리 후 검사’로 전환\n",
      "12 목포서 붕어빵 판매상 부부 코로나 확진… 전남 8명으로 늘어\n",
      "13 소라넷이 시초, 솜방망이 처벌이 만든 디지털 성범죄\n",
      "14 이재명 “모든 경기도민에 재난기본소득 10만원씩”\n",
      "15 보수 커뮤니티, 봉사, 성착취범  조주빈의 비열한 행보\n"
     ]
    }
   ],
   "source": [
    "# 크롤링 데이터 엑셀 파일 읽기 \n",
    "import openpyxl\n",
    "\n",
    "excel_file = openpyxl.load_workbook('한국일보 사회뉴스 타이틀 크롤링.xlsx')\n",
    "excel_sheet = excel_file.active\n",
    "# excel_sheet = excel_file.get_sheet_by_name('사회뉴스') - 여러개 시트가 있을 때\n",
    "\n",
    "for row in excel_sheet.rows:\n",
    "    print(row[0].value, row[1].value)  # row의 첫번째, 두번째 칼럼 추출\n",
    "\n",
    "excel_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 과제: 한국일보 사회 지면 5 페이지 크롤링 \n",
    "\n",
    "여기서 과젭니다! 우리는 1페이지만 크롤링을 해봤어요. 그런데 그 이상 여러개의 쪽들을 모두 크롤링하려면 어떻게 해야 할까요? **한국일보 사회면의 1쪽부터 5쪽까지 타이틀만 크롤링해 엑셀 파일에 저장**하는 것이 여러분들에게 드리는 과제입니다! 만든 엑셀 파일을 구글 드라이브 과제방에 올려주시면 됩니다. \n",
    "\n",
    "- 힌트: https://www.hankookilbo.com/News/Society/HC01?Page=1 여기 page에 할당된 숫자를 변경해보세요. 변경한 후 어떤 페이지가 나오는지 생각해보세요. \n",
    "\n",
    "힌트 이외에는 여러분이 프로그래밍으로 충분히 해결할 수 있으니 고민해보세요😁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 정규표현식 \n",
    "\n",
    "정규 표현식(Regular Expressions)은 복잡한 문자열을 처리할 때 사용하는 기법으로, 파이썬만의 고유 문법이 아니라 문자열을 처리하는 모든 곳에서 사용해요. 기초 문법 할 때는 다루지 않았는데요, 그만큼 초보들이 배우기에는 어려울 수 있어요. 하지만 우리는 때가 되었으니! 정규표현식에 대해 배워봅시다. \n",
    "\n",
    "- [점프 투 파이썬 7장 정규표현식](https://wikidocs.net/1669) - 7장을 모두 숙지하시면 됩니다. \n",
    "- 위를 학습한 후, 구글 드라이브의 **regularexpression.html**을 학습하세요. 그 안에 구체적인 과제들이 담겨 있습니다. \n",
    "- **두번째 교안의 과제들**까지 모두 완성해 해당 구글 콜랩 or 쥬피터 html 파일을 과제로 제출해주시면 됩니다. \n",
    "\n",
    "### 공부를 끝내며 \n",
    "\n",
    "어떤가요? 이번 주는 뭔가 새로운 것들이 많았을 것 같은데요, 웹이 이렇게 구성되는 구나에 대해서도 조금이나마 알게 되었고, 신기한 웹 크롤링까지 해보게 되었네요💗 우리가 배운 웹 크롤링은 정말 기초적인 것에 불과하니, 좀 더 연습해보시면서 원하는 것을 크롤링하려고 하시되, 불법적인 방식으로는 사용하면 안됩니다. 또한 정규표현식도 배웠죠? 정말 외계어 같지 않나요ㅎㅎ 이것을 이제 이해할 수 있게 된 여러분은 외계어도 정복하셨습니다! \n",
    "\n",
    "이번 주차는 다음 프로젝트 '시카고 샌드위치 맛집'을 잘 해나가고, 앞으로 나올 웹크롤링에 잘 대처하기 위한 것이니 앞으로의 과제에 도움이 많이 될 것 같습니다ㅎㅎ 그럼 고생하셨고, 다음 프로젝트에서 다시 만나요!💖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
