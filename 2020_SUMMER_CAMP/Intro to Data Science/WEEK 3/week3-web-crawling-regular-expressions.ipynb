{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🦉 COSADAMA  Introduction to Data Science Study\n",
    "* 일자: 2020-07-22\n",
    "* 작성자: 김가윤\n",
    "* 참고자료: 파이썬으로 데이터 주무르기(민형기), 점프 투 파이썬(박응용), 파이썬 입문과 크롤링 기초 부트캠프(잔재미코딩), COSADAMA Spring Rush Introduction to Data Science(박하람)\n",
    "* 교재: 125 - 138쪽 (깃헙 교안으로 공부하면서 교재를 참고 삼아, 복습 겸 읽어보세요~)\n",
    "* 학습키워드: 웹크롤링, 정규표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 웹 크롤링과 정규표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습 목표\n",
    "    1. **BeatifulSoup과 urllib 라이브러리를 활용해 웹 크롤링**을 해보고, 엑셀을 열고 기록하는데 사용되는 모듈 openpyxl을 사용해 가져온 타이틀들을 엑셀 파일에 저장하고 읽어 봅니다.\n",
    "    2. **정규표현식을 통해 원하는 데이터를 추출**합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 웹 크롤링과 정규표현식이란?\n",
    "\n",
    "웹 크롤링이라는 것은 웹에서 내가 원하는 데이터를 긁어오는 것을 말합니다. 웹 크롤링 기술에도 여러가지가 있기 때문에 이번 시간에는 기초적인 기술을 먼저 배워볼거에요! 크롤링한 데이터를 내가 원하는 부분만 가져오기 위해 정규표현식이 쓰이게 됩니다 😋😋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 HTML/CSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "웹 크롤링을 하려면 웹이 어떤 문법으로 만들어지는지부터 우선 알아야 합니다. html는 웹 페이지를 만들기 위한 언어로 웹브라우저 위에서 동작하는 언어에요. 웹 페이지 뿐만 아니라 이미지, 텍스트, 비디오의 내용도 담을 수 있습니다.  CSS는 간단히 웹 브라우저를 꾸며주는 언어라고 생각하면 되구요. 웹이 어떻게 구성되어 있는지를 이해해야 웹 크롤링도 할 수 있어요. 아래 링크를 보고 해당 부분만 학습하고 오면 됩니다! \n",
    "\n",
    "* [Introduction to HTML](https://www.codecademy.com/learn/learn-html) - syllabus 1. elements & structured의 Introduction to HTML과 HTML Document Standard 학습하기\n",
    "* [Learn CSS](https://www.codecademy.com/learn/learn-css) - syllabus 1. Selectors and Visual Rules의 CSS Setup & Selectors 학습하기\n",
    "\n",
    "처음 배우시면 이것을 끝내시는데 하루(3시간 이상) 정도는 걸릴 것 같아요. html/css를 아시는 분들은 이 파트를 넘어가시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 웹 데이터를 가져오는 BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup은 웹 페이지의 내용을 가져오기 위해 사용하는 모듈이에요. \n",
    "[BeautifulSoup에 대한 자세한 내용](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)은 링크를 클릭해주세요! \n",
    "\n",
    "우선 구글 드라이브의 **02. test_first.html**을 다운받아 직접 학습해봅시다.\n",
    "먼저 BeautifulSoup을 import 해줄게요~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 다운 받은 파일을 열어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html>\n",
      "<head>\n",
      "<title>Very Simple HTML Code by PinkWink</title>\n",
      "</head>\n",
      "<body>\n",
      "<div>\n",
      "<p class=\"inner-text first-item\" id=\"first\">\n",
      "                Happy PinkWink.\n",
      "                <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
      "</p>\n",
      "<p class=\"inner-text second-item\">\n",
      "                Happy Data Science.\n",
      "                <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
      "</p>\n",
      "</div>\n",
      "<p class=\"outer-text first-item\" id=\"second\">\n",
      "<b>\n",
      "                Data Science is funny.\n",
      "            </b>\n",
      "</p>\n",
      "<p class=\"outer-text\">\n",
      "<b>\n",
      "                All I need is Love.\n",
      "            </b>\n",
      "</p>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "page = open(\"C:\\\\Users\\\\Owner\\\\Documents\\\\GitHub\\\\cosadama\\\\SUMMER CAMP 2020\\\\INTRO TO DS\\\\03. test_first.html\", \"r\").read()\n",
    "# 본인 파일 경로 넣기\n",
    "soup = BeautifulSoup(page, 'html.parser') # Python’s html.parser - 문서 전체를 저장한 변수 \n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "페이지를 이루고 있는 html의 구조가 불러져왔어요~ 더 예쁘게 보고 싶다면 `soup.prettify()`를 이용하면 됩니다.\n",
    "\n",
    "* `soup.prettify()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Very Simple HTML Code by PinkWink\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <div>\n",
      "   <p class=\"inner-text first-item\" id=\"first\">\n",
      "    Happy PinkWink.\n",
      "    <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">\n",
      "     PinkWink\n",
      "    </a>\n",
      "   </p>\n",
      "   <p class=\"inner-text second-item\">\n",
      "    Happy Data Science.\n",
      "    <a href=\"https://www.python.org\" id=\"py-link\">\n",
      "     Python\n",
      "    </a>\n",
      "   </p>\n",
      "  </div>\n",
      "  <p class=\"outer-text first-item\" id=\"second\">\n",
      "   <b>\n",
      "    Data Science is funny.\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"outer-text\">\n",
      "   <b>\n",
      "    All I need is Love.\n",
      "   </b>\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())  # 태그를 구분하기 쉽게 해주는 명령"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['html',\n",
       " '\\n',\n",
       " <html>\n",
       " <head>\n",
       " <title>Very Simple HTML Code by PinkWink</title>\n",
       " </head>\n",
       " <body>\n",
       " <div>\n",
       " <p class=\"inner-text first-item\" id=\"first\">\n",
       "                 Happy PinkWink.\n",
       "                 <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       " </p>\n",
       " <p class=\"inner-text second-item\">\n",
       "                 Happy Data Science.\n",
       "                 <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       " </p>\n",
       " </div>\n",
       " <p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>\n",
       " <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>\n",
       " </body>\n",
       " </html>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(soup.children) # soup에서 children(한단계 아래인 애들) 인 부분을 묶어서 리스트로 만들어줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head>\n",
       "<title>Very Simple HTML Code by PinkWink</title>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "<p class=\"inner-text first-item\" id=\"first\">\n",
       "                Happy PinkWink.\n",
       "                <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       "</p>\n",
       "<p class=\"inner-text second-item\">\n",
       "                Happy Data Science.\n",
       "                <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       "</p>\n",
       "</div>\n",
       "<p class=\"outer-text first-item\" id=\"second\">\n",
       "<b>\n",
       "                Data Science is funny.\n",
       "            </b>\n",
       "</p>\n",
       "<p class=\"outer-text\">\n",
       "<b>\n",
       "                All I need is Love.\n",
       "            </b>\n",
       "</p>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = list(soup.children)[2] # 리스트의 2번째 요소를 불러와줘\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " <head>\n",
       " <title>Very Simple HTML Code by PinkWink</title>\n",
       " </head>,\n",
       " '\\n',\n",
       " <body>\n",
       " <div>\n",
       " <p class=\"inner-text first-item\" id=\"first\">\n",
       "                 Happy PinkWink.\n",
       "                 <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       " </p>\n",
       " <p class=\"inner-text second-item\">\n",
       "                 Happy Data Science.\n",
       "                 <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       " </p>\n",
       " </div>\n",
       " <p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>\n",
       " <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>\n",
       " </body>,\n",
       " '\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(html.children) # html의 자식들(밑의 애들)을 가져와줘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* html 파일의 body만 뽑는 법\n",
    "\n",
    "    1. list로 선언해서 안의 내용부터 출력하기\n",
    "    2. soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<div>\n",
       "<p class=\"inner-text first-item\" id=\"first\">\n",
       "                Happy PinkWink.\n",
       "                <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       "</p>\n",
       "<p class=\"inner-text second-item\">\n",
       "                Happy Data Science.\n",
       "                <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       "</p>\n",
       "</div>\n",
       "<p class=\"outer-text first-item\" id=\"second\">\n",
       "<b>\n",
       "                Data Science is funny.\n",
       "            </b>\n",
       "</p>\n",
       "<p class=\"outer-text\">\n",
       "<b>\n",
       "                All I need is Love.\n",
       "            </b>\n",
       "</p>\n",
       "</body>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = list(html.children)[3] # 바디만 따로 담을래\n",
    "body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<div>\n",
       "<p class=\"inner-text first-item\" id=\"first\">\n",
       "                Happy PinkWink.\n",
       "                <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       "</p>\n",
       "<p class=\"inner-text second-item\">\n",
       "                Happy Data Science.\n",
       "                <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       "</p>\n",
       "</div>\n",
       "<p class=\"outer-text first-item\" id=\"second\">\n",
       "<b>\n",
       "                Data Science is funny.\n",
       "            </b>\n",
       "</p>\n",
       "<p class=\"outer-text\">\n",
       "<b>\n",
       "                All I need is Love.\n",
       "            </b>\n",
       "</p>\n",
       "</body>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.body # 바로 body 보여줘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `soup.find_all()` : 모두 찾아줘\n",
    "* `soup.find()` : 하나만 찾아줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"inner-text first-item\" id=\"first\">\n",
       "                 Happy PinkWink.\n",
       "                 <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       " </p>,\n",
       " <p class=\"inner-text second-item\">\n",
       "                 Happy Data Science.\n",
       "                 <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       " </p>,\n",
       " <p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>,\n",
       " <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p')   # p 태그를 모두 찾아줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>,\n",
       " <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p', class_='outer-text')  # p 중 class가 outer-text인 태그만 모두 가져와줘\n",
    "#class가 아닌 점 유의!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* class 명으로 태그 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>,\n",
       " <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(class_='outer-text') # class가 outer-text인 것을 모두 찾아줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"inner-text first-item\" id=\"first\">\n",
       "                 Happy PinkWink.\n",
       "                 <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       " </p>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(id=\"first\")   #id가 first인 것을 모두 찾아줘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `soup.head()` : head 내용 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>Very Simple HTML Code by PinkWink</title>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.head.next_sibling # soup.head의 다음 애"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<div>\n",
       "<p class=\"inner-text first-item\" id=\"first\">\n",
       "                Happy PinkWink.\n",
       "                <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       "</p>\n",
       "<p class=\"inner-text second-item\">\n",
       "                Happy Data Science.\n",
       "                <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       "</p>\n",
       "</div>\n",
       "<p class=\"outer-text first-item\" id=\"second\">\n",
       "<b>\n",
       "                Data Science is funny.\n",
       "            </b>\n",
       "</p>\n",
       "<p class=\"outer-text\">\n",
       "<b>\n",
       "                All I need is Love.\n",
       "            </b>\n",
       "</p>\n",
       "</body>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.head.next_sibling.next_sibling # soup.head의 다음 다음 애"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* html의 텍스트 정보 추출하기\n",
    "\n",
    "    ex. 'p' 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"inner-text first-item\" id=\"first\">\n",
       "                 Happy PinkWink.\n",
       "                 <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       " </p>,\n",
       " <p class=\"inner-text second-item\">\n",
       "                 Happy Data Science.\n",
       "                 <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       " </p>,\n",
       " <p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>,\n",
       " <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p') # [] 안에 내용이 있는 것을 보니 리스트 형태~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Happy PinkWink.\n",
      "                PinkWink\n",
      "\n",
      "\n",
      "                Happy Data Science.\n",
      "                Python\n",
      "\n",
      "\n",
      "\n",
      "                Data Science is funny.\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "                All I need is Love.\n",
      "            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each_tag in soup.find_all('p'): # p를 모두 찾은 리스트 안을 도는 동안\n",
    "    print(each_tag.get_text())  # 태그 안의 텍스트만 가져와줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n                Happy PinkWink.\\n                PinkWink\\n\\n\\n                Happy Data Science.\\n                Python\\n\\n\\n\\n\\n                Data Science is funny.\\n            \\n\\n\\n\\n                All I need is Love.\\n            \\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body.get_text()  # 태그가 있는 자리는 줄바꿈이 되고 전체 텍스트를 보여줘요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ex. 'a' 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>,\n",
       " <a href=\"https://www.python.org\" id=\"py-link\">Python</a>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = soup.find_all('a')\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PinkWink\n",
      "PinkWink -> http://www.pinkwink.kr\n",
      "Python\n",
      "Python -> https://www.python.org\n"
     ]
    }
   ],
   "source": [
    "for each in links: \n",
    "    href = each['href'] # href만 가져와\n",
    "    text = each.string # each에 있는 문자열\n",
    "    print(text + ' -> ' + href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 urllib 라이브러리를 이용해 웹 크롤링하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 앞서 배운 BeautifulSoup 모듈과 urllib 라이브러리를 활용해 실제 사이트에서 웹을 크롤링해볼게요~! \n",
    "\n",
    "* **urllib** 라이브러리란 ?\n",
    "\n",
    "urllib은 URL 처리에 관련된 모듈을 모아 놓은 패키지입니다. urllib에 있는 request 모듈을 가져오고, 그 안의 함수를 사용할거에요. urllib.request.urlopen은 URL을 여는 함수인데 URL 열기에 성공하면 response.status의 값이 200이 나옵니다. 이 200은 HTTP 상태 코드이며 웹 서버가 요청을 제대로 처리했다는 뜻입니다. [더 자세한 내용은 클릭](https://docs.python.org/ko/3/library/urllib.html)\n",
    "\n",
    "네이버 증권 국내증시에서 오늘의 코스피를 가져와 볼겁니다.\n",
    "\n",
    "먼저 코스피에 해당하는 텍스트의 태그, 아이디, 클라스 등을 알아야겠죠? **크롬의 개발자 도구**를 활용하면 쉽게 내가 가져오고자 하는 텍스트의 태그와 아이디, 클라스를 알 수 있어요. 교재 136-138쪽을 보시면 아실 수 있습니다. 혹은 인터넷을 사용해도 되구요.\n",
    "\n",
    "   * **크롬 개발자 도구**?\n",
    "    구글에서 만든 웹브라우저인 크롬에는 개발을 도와주는 다양한 도구가 기본적으로 제공됩니다. \n",
    "    [크롬 개발자 도구 사용법](https://mainia.tistory.com/2393) 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [네이버 금융 국내증시](https://finance.naver.com/sise/)\n",
    "\n",
    "코스피를 크롬 개발자 도구로 찾아보니까 `<span>`에 대한 태그들을 가져와야 겠네요. 그리고 `KOSPI_now` id를 가지고 있어요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[<span class=\"num \" id=\"KOSPI_now\">2,217.86</span>, <span class=\"num \" id=\"KOSDAQ_now\">801.23</span>, <span class=\"num \" id=\"KPI200_now\">293.51</span>]\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen # request 모듈에서 urlopen import 할게~\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = urlopen('https://finance.naver.com/sise/') # url 넣기\n",
    "print(res.status) # 200이 나오면 성공~\n",
    "soup = BeautifulSoup(res, \"html.parser\") # 객체 생성\n",
    "\n",
    "data = soup.find_all('span', class_='num') # 원하는 태크를 이용해 찾기\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data의 형태가 리스트인 것을 아시겠나요?! 코스피만 가져오려면 가장 첫 항목만 가져오면 되겠죠? 출력해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"num \" id=\"KOSPI_now\">2,217.86</span>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kospi = data[0]\n",
    "kospi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2,217.86'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kospi.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_text()를 이용해 코스피만 딱 가져왔네요 :) ~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `get.text()`\n",
    "\n",
    ": 텍스트 내용만 가져와주는 bs4의 메서드입니다.\n",
    "\n",
    "If you only want the human-readable text inside a document or tag, you can use the get_text() method. It returns all the text in a document or beneath a tag, as a single Unicode string.\n",
    "\n",
    "cf. 아까는 함수가 나오고...지금은 메서드...? [호옥시 메서드와 함수의 차이점이 궁금한 분들을 위하여](https://velog.io/@qoszino/%ED%95%A8%EC%88%98%EC%99%80-%EB%A9%94%EC%86%8C%EB%93%9C%EC%9D%98-%EC%B0%A8%EC%9D%B4%EC%A0%90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 뉴스 기사 타이틀 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 뉴스 기사 타이틀 크롤링하는 것에 대해 알아보려고 해요. 한국일보의 사회 면의 1페이지부터 5페이지까지의 뉴스 타이틀만 가져와볼겁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [한국일보 사회](https://www.hankookilbo.com/News/Society)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 모듈을 import 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen # request 모듈에서 urlopen import 할게~\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.hankookilbo.com/News/Society/HC01'\n",
    "page = urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soup의 내용을 그냥 print하면 내용이 엄청 많아요. 저희가 찾고자하는 타이틀이 어떤 태그와 클라스를 가지는지 찾아봅시다. 아까 알려드린 구글 개발자 도구를 사용해보세요! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[인사] 한국전기안전공사 외', '아파트 잠기고 자동차 둥둥... 대전·충청 비피해 속출', '[부고] 장석희씨 별세 外', \"경찰, 조직분리 없이 '한지붕 세가족'... '몸집 줄이기' 취지 무색\", '여성단체 \"인권위 \\'박원순 의혹\\' 직권조사로 진상규명 기대\"', \"인권위 '직권조사' 만장일치 결정... 박원순 성추행 의혹 실체 밝힌다\", '장애아동 팔 꺾어 강제로 밥 먹여...특수학교 교사 학대 정황', '\\'구급차 막은 택시기사\\'…유족 측 \"살인이다\" 추가 고소', '[속보] 인권위, 박원순 성추행 의혹 직권조사 결정', '‘물폭탄’ 충북서도 침수ㆍ산사태 속출']\n"
     ]
    }
   ],
   "source": [
    "title = soup.find_all('h3')\n",
    "titles=[]\n",
    "\n",
    "for n in title:\n",
    "    a = n.get_text()\n",
    "    titles.append(a)\n",
    "\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제가 교안을 쓸 때는 제목 리스트를 보다보니 \\xa0와 같은 이상한 문자들이 섞여 나왔는데 새로 업데이트 한 오늘(7월 30일)은 바꿔줄 문자가 없네요.... 아무튼 혹시라도 제목에 이상한 문자들이 있다면 이것들은 제목이 아니니 처리해줘야겠죠? 이럴 때 사용하는 것이 바로 **정규표현식** 이랍니다~ 이번 주차를 다 공부하고 나면 제목만 쏙쏙 뽑아낼 수 있을거에요. 더 정확한 예시는 첨부한 구글 드라이브의 파일들과 예시를 살펴봅시다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 정규표현식\n",
    "정규 표현식(Regular Expressions)은 복잡한 문자열을 처리할 때 사용하는 기법으로, 파이썬만의 고유 문법이 아니라 문자열을 처리하는 모든 곳에서 사용해요. 기초 문법 할 때는 다루지 않았는데요, 그만큼 초보들이 배우기에는 어려울 수 있어요. 하지만 우리는 때가 되었으니! 정규표현식에 대해 배워봅시다.\n",
    "\n",
    "[점프 투 파이썬 7장 정규표현식](https://wikidocs.net/1669) - 7장을 모두 숙지하시면 됩니다.\n",
    "\n",
    "위를 학습한 후, **구글 드라이브의 regularexpression.html**을 학습하세요. 그 안에 구체적인 과제들이 담겨 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1 왜 정규표현식이 필요할까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\n                                        박지원 청문보고서 민주당 단독 채택…통합당은 ‘부적격’ 불참\\n                                    \\n', '\\n\\n                                        세계 최초 부생수소 발전소 준공…\"추가 오염물질 없는 친환경\"(종합)\\n                                    \\n', '\\n\\n                                        \\'상견례\\'서 충돌한 여야…통합 \"관행 절차 다 무시, 상임위 여당 독재\"\\n                                    \\n', '\\n\\n                                        손병두 \"아시아나항공 국유화 가능성도 감안\" 언급에 주가 급등\\n                                    \\n', '\\n\\n                                        김정은 \"우린 핵보유국\" 과시…이인영 \"北, 비핵화 의지 있어\"\\n                                    \\n']\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "res = urlopen('https://news.naver.com/')\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "data = soup.find_all('div', class_='hdline_article_tit')\n",
    "alist=[]\n",
    "for title in data:\n",
    "    a= title.get_text()\n",
    "    alist.append(a)\n",
    "    \n",
    "print(alist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['우주발사체 고체연료 제한 해제, 사실상 ICBM 개발 허용    ', '박지원 청문보고서 민주당 단독 채택…통합당은 ‘부적격’ 불참    ', '세계 최초 부생수소 발전소 준공…\"추가 오염물질 없는 친환경\"(종합)    ', '\\'상견례\\'서 충돌한 여야…통합 \"관행 절차 다 무시, 상임위 여당 독재\"    ', '김정은 \"우린 핵보유국\" 과시…이인영 \"北, 비핵화 의지 있어\"    ']\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "res = urlopen('https://news.naver.com/')\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "data = soup.find_all('div', class_='hdline_article_tit')\n",
    "alist=[]\n",
    "for title in data:\n",
    "    a= re.sub('[\\\\n]','',title.get_text())\n",
    "    b= re.sub(\"\\s\\s\\s\\s\\s\\s\\s\\s\",'', a)\n",
    "    alist.append(b)\n",
    "    \n",
    "print(alist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규표현식으로 타이틀을 정리해주니 말끔하게 제가 원하던 헤드라인 기사 제목만 뽑혔네요. 어 그런데 상견례에 보이는 \\는 안 없어진거 아닌가요? 하실 수 있겠지만 리스트의 요소를 나타낼 때 사용하는 작은 따옴표와 기사 제목의 작은 따옴표를 구분하기 위해 써준거라고 보시면 될거 같아요. 출력해보면 원래대로 나온답니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 openxypl 모듈로 웹 크롤링 자료 저장, 읽기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크롤링 했다면 그 결과를 담아야 나중에도 또 쓸 수 있겠죠? 후에 판다스에 연결해서 엑셀을 읽어낼 수도 있구요. 이렇게 엑셀에 무언가를 담기 위해 필요한 모듈은 openpyxl이라는 것인데요, 아래 코드를 한번만 이해한다면 계속해서 활용하실 수 있을 거에요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 데이터로 엑셀파일 만들기 \n",
    "\n",
    "import openpyxl\n",
    "\n",
    "excel_file = openpyxl.Workbook()\n",
    "excel_file.remove(excel_file.active)\n",
    "excel_sheet = excel_file.create_sheet('안녕 시트')  # sheet 이름 작성 \n",
    "\n",
    "excel_sheet.column_dimensions['B'].width = 150 # column B 크기 정하기 \n",
    "\n",
    "for index in range(9):\n",
    "    excel_sheet.append([index, '안녕']) # 엑셀파일에서 이게 어떻게 구현됐을까요? \n",
    "\n",
    "cell_A1 = excel_sheet['A1']\n",
    "cell_A1.alignment = openpyxl.styles.Alignment(horizontal=\"center\")\n",
    "\n",
    "excel_file.save('test.xlsx')  # 엑셀 파일 이름 설정\n",
    "excel_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일이 저장된 폴더에 가보세요. 그렇다면 test.xlsx 파일이 만들어져 있을 것이랍니다!\n",
    "\n",
    "우리 앞에서 네이버 뉴스 타이틀을 추출한 것을 엑셀에다 담아볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen \n",
    "from bs4 import BeautifulSoup\n",
    "import openpyxl\n",
    "\n",
    "excel_file = openpyxl.Workbook()\n",
    "excel_file.remove(excel_file.active)\n",
    "excel_sheet = excel_file.create_sheet('헤드라인')  # sheet 이름 작성 \n",
    "excel_sheet.column_dimensions['B'].width = 100   # 컬럼 크기 정하기 \n",
    "\n",
    "excel_sheet.append(['번호','제목']) #sheet에 표제목 넣기 \n",
    " \n",
    "url = 'https://news.naver.com/'\n",
    "page = urlopen(url)\n",
    "soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "num = 0\n",
    "data = soup.find_all('div', class_='hdline_article_tit')   # 각 타이틀에서 div 중 class가 hdline_article_tit인 것 가져오기 \n",
    "for title in data: \n",
    "    a= re.sub('[\\\\n]','',title.get_text())\n",
    "    b= re.sub(\"\\s\\s\\s\\s\\s\\s\\s\\s\",'', a)\n",
    "    num += 1\n",
    "    excel_sheet.append([num, b])  # 타이틀 개수와 타이틀 내용\n",
    "\n",
    "\n",
    "cell_A1 = excel_sheet['A1']\n",
    "cell_A1.alignment = openpyxl.styles.Alignment(horizontal=\"center\")  # A1 양식 center로!\n",
    "cell_B1 = excel_sheet['B1']\n",
    "cell_B1.alignment = openpyxl.styles.Alignment(horizontal=\"center\")  # B1 양식 center로!\n",
    "\n",
    "excel_file.save('네이버 뉴스 헤드라인 타이틀 크롤링.xlsx')\n",
    "excel_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**과제 1: 한국일보 사회 지면 5 페이지 크롤링**  \n",
    "\n",
    "**선택 + 클릭하면 기사로 이동하는 URL도 같이 엑셀 파일에 담아 보세요!**\n",
    "\n",
    "여기서 과젭니다! 교안에서는 네이버 뉴스 헤드라인 타이틀만 크롤링을 해봤어요. 그런데 그 이상 여러개의 쪽들을 모두 크롤링하려면 어떻게 해야 할까요? 아까 예시로 공부했던 한국일보 사회면의 1쪽부터 5쪽까지 타이틀만 크롤링해 엑셀 파일에 저장하는 것이 여러분들에게 드리는 과제입니다! 엇 그런데 예시에서도 보았듯이 크롤링한 데이터에 xa0와 같이 불필요한 요소들이 있었죠? 이를 정규표현식으로 제거해서 만든 엑셀 파일을 구글 드라이브 과제방에 올려주시면 됩니다 ~~~ 과제 1이 너무 쉽다! 하시는 분들은 클릭하면 기사가 나오는 url 부분도 추출해서 담아보세요. 시카고 샌드위치 프로젝트까지 하고 오시면 더욱 쉽게 하실 수 있을 거에요 😉\n",
    "\n",
    "힌트 1: https://www.hankookilbo.com/News/Society/HC01?Page=1 여기 page에 할당된 숫자를 변경해보세요.   \n",
    "변경한 후 어떤 페이지가 나오는지 생각해보세요.   \n",
    "힌트 2: https://regex101.com/ 정규표현식을 확인할 수 있는 홈페이지에서 크롤링을 통해 나온 타이틀들을 복사한뒤 직접 확인해보세요. \n",
    "힌트 이외에는 여러분이 프로그래밍으로 충분히 해결할 수 있으니 고민해보세요😁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**과제 2: regularexpression.html 맨 뒷부분의 도전 과제 2개**\n",
    "\n",
    "과제 1과 2를 모두 완성해 해당 구글 콜랩 or 쥬피터 html 파일을 과제로 제출해주시면 됩니다.😆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 공부를 마치며 🦒"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 주차 내용 어떠셨나요? 신기하지 않나요? ㅎㅎㅎ 이번 차시를 통해 저희는 웹의 구성과 함께 기초적인 크롤링 방법도 알아보았어요. 또 크롤링한 데이터 중 예쁘게 저희가 원하는 것만 뽑아보기 위해 사용하는 정규표현식도 배웠습니다. 이는 다음 프로젝트 '시카고 샌드위치 맛집'을 잘 이해하기 위한 디딤돌이 될거랍니다. 정말 수고 많으셨고, 질문은 우리 모두가 성장할 수 있는 거름이 됩니다 😻 주저하지 마시고 부끄러워 하지마시고 마음껏 해주세요 ~ 항상 대✨환✨영입니다!! 😎 날씨가 더운데 몸조리 잘하시고, 우리는 시카고의 샌드위치 맛집을 찾으러 또 만납시당. 안뇽 ~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
