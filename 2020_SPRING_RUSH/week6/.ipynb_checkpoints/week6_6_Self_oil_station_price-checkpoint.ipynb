{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🦉COSADAMA Introduction to Data Science Study\n",
    "\n",
    "- 일자: 2020-05-11\n",
    "- 작성자: 박하람 \n",
    "- 참고자료: 파이썬으로 데이터 주무르기(민형기)\n",
    "- 교재: 169 - 198쪽 (깃헙 교안으로 공부하면서, 궁금한 부분은 교재를 참고하세요. 혹은 깃헙 교안을 다 보고 교재를 복습 겸 읽어봐도 됩니다!)\n",
    "\n",
    "## 6. 셀프 주유소는 정말 저렴할까\n",
    "\n",
    "모두들 중간고사는 무사히 잘 치르셨나요? 스스로 해보는 데이터 분석 과제도 어떤 자료를 가지고 하셨는지 무척 궁금하네요😊 오늘은 중간고사 전 마지막 주차에서 배운 웹 크롤링 BeautifulSoup을 다시 활용해볼 것이구요, urllib 라이브러리 대신 Selenium을 사용해 urllib으로 크롤링이 불가능한 사이트를 크롤링해보려 합니다. 더불어 중간고사 이전부터 학습한 pandas, Google Maps, Folium과 Seaborn 등도 모두 다시 활용해 볼 것이에요. 좋은 리뷰가 될 것 같죠?💖\n",
    "\n",
    "이번 프로젝트는 '셀프 주요소는 정말로 저렴한지'를 팩트 체크해보는 것입니다. 한국석유공사 오피넷에서 Selenium으로 데이터를 크롤링해볼 것이구요, 엑셀파일에 담아 glob 모듈을 활용해 다수의 엑셀파일을 위치를 찾아 읽어보는 것까지 해볼 겁니다. 또한 박스 플롯으로 시각화해 실제로 셀프 주유소가 저렴한지 알아보고, 대체적으로 휘발유 값이 저렴한 구는 어디인지 Google Maps와 Folium으로 시각화해볼 겁니다.(익숙하죵?ㅎㅎ) 마지막으로는 주유소 가격 상하위 10개를 뽑아 지도에 시각화해 볼 것이랍니다. 이번 주차도 매우 흥미로운 주제이나, 다만 Selenium을 활용하는 것이 어려울 수 있어요. 그러니 궁금한 것이나 막히는 부분이 있다면 언제든 슬랙으로 질문해주세요! 그럼 이번 주차 공부도 시작합니다🐻\n",
    "\n",
    "### 6.1 Selenium 사용하기\n",
    "\n",
    "지난 시간에는 urllib을 활용해서 정보를 가져왔어요. 하지만 이번에는 urllib 라이브러리를 사용해 크롤링할 수 없기 때문에 이 때 사용하는 Selenium를 배워볼 것이에요. 우선, 우리가 주유소 가격과 위치를 끌어오려면, 주유소 정보들이 있는 opinet 사이트를 이용해보려 합니다. \n",
    "\n",
    "- [opinet - 싼 주유소 찾기 - 지역별](http://www.opinet.co.kr/searRgSelect.do): 이 페이지를 선택하면 opinet 사이트가 나올 것이에요. 여기에서 싼 주유소 찾기를 누르고, 지역별을 누르면 우리가 얻고자 하는 사이트가 나와요. \n",
    "\n",
    "그런데 문제는 이렇게 지역별 사이트에 들어가도, url 주소가 달라지지 않았기 때문에 해당 url을 복사해 다시 로드하면 Opinet 처음 사이트로 돌아가게 된답니다. 그럴 때에는 url 주소만 있으면 되는 urllib를 사용할 수 없겠죠. 그렇게 해서 사용하는 것이 바로 Selenium입니다.\n",
    "\n",
    "#### Selenium 설치와 웹 드라이버 \n",
    "\n",
    "Selenium은 터미널에 pip로 설치하시면 됩니다. 그러나 Selenium을 사용하기 위해서는 사용하는 브라우저에 맞춰 웹 드라이버를 다운 받아야 해요. 본인이 사파리를 쓰시면 사파리 웹 드라이버를, 크롬을 쓰시는 크롬 웹 드라이버를 쓰시면 돼요. 저는 크롬 드라이버가 더 깔기 편해서 크롬 드라이버를 추천드려요. \n",
    "\n",
    "- pip install selenium 터미널에 입력 \n",
    "- [본인에게 맞는 웹브라우저 별로 드라이브 설치 - 경로설정 주의](https://sacko.tistory.com/13)\n",
    "\n",
    "웹 브라우저를 다운받으신 후, data나 소스코드가 있는 폴더에 drive를 압축 해제해주시면 된답니다.(교재 171쪽) 이 때 파일경로가 중요한데요, 파일경로를 확인하는 방법은 아주 간단하게 Terminal(명령 프롬프트)을 열고 drive 파일을 터미널에 끌어 올려다 넣으시면 이동경로가 나타나게 됩니다. 아래 분홍색이 이동경로에요.\n",
    "\n",
    "![파일 경로](file_path_img.png)\n",
    "\n",
    "이동경로를 기억해 두시고 먼저 selenium을 import 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이동경로를 아래에다 넣어주고, get에다가는 원하는 사이트의 주소를 입력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('/Users/harampark/Documents/chromedriver') # 나의 이동경로라 복붙 X\n",
    "driver.get(\"https://nid.naver.com/nidlogin.login?mode=form&url=https%3A%2F%2Fwww.naver.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selenium으로 Naver 로그인하기 \n",
    "\n",
    "어, 그러니까 갑자기 새로운 화면창이 떴죠? 위에 'Chrome이 자동화된 테스트 소프트웨어에 의해 제어되고 있습니다.'라고 하는게 뜨면 잘 된 것이랍니다. 웬만하면 이 화면은 직접 건들이지 않고, Selenium이 만지게 하는게 좋아요.(코드를 작성할 때 혼선이 생길 수 있기 때문) \n",
    "\n",
    "스크린샷을 하고 싶다면 아래와 같은 명령을 사용하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스크린샷 - 파일 경로 확인, 확장자 확인, 미리 폴더 만들어놓아야 함\n",
    "driver.save_screenshot('001.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![파일 경로](images/001.png)\n",
    "\n",
    "이렇게 나오면 잘 캡처가 되었다고 할 수 있죠. 그럼 이제 Selenium으로 로그인을 해보도록 할게요. 로그인을 하려면, 크롬 개발자 도구를 사용해 아이디와 패스워드가 적힌 곳의 id를 알아야 해요. 잘 기억이 안난다면 136-138쪽을 보시구요, 이번에는 find_element_by_id를 사용해서 id를 찾는 것이랍니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem_login = driver.find_element_by_id(\"id\")\n",
    "elem_login.clear()  # 미리 입력된 글자를 지워주는 것(자동로그인 경우)\n",
    "elem_login.send_keys(\"********\")\n",
    "\n",
    "elem_login = driver.find_element_by_id(\"pw\")\n",
    "elem_login.clear()\n",
    "elem_login.send_keys(\"**********\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![파일 경로](images/002.png)\n",
    "\n",
    "이렇게 스스로 적혀져 있다면 잘 하셨습니다. 그 다음에는 로그인을 눌러야 클릭이 되잖아요? 그 클릭하는 방법은 xpath를 찾아서 클릭하는 방법이에요. 교재 174-175쪽을 보면 xpath copy를 하는 방법을 알 수 있어요. 그렇게 복사한 xpath을 아래에다 적어주시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = \"\"\"//*[@id=\"log.login\"]\"\"\"\n",
    "driver.find_element_by_xpath(xpath).click() # xpath 위치를 클릭해라"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![003](images/003.png)\n",
    "\n",
    "엇, 로그인을 했는데 이렇게 막히네요. 이게 바로 캡차라는 기술인데요, 요렇게 자동으로 로그인이 되는 것을 막기 위해서 컴퓨터를 이용해 로그인 했다고 의심이 되는 로그인은 캡차로 인증하게 합니다. 네이버에서 매크로로 댓글 조작을 했니 어쨌니 이런 논란이 많았잖아요. 네이버는 아무래도 사측에서 암묵적으로 가담했다는 증거를 피하기 위해서 혹은 해킹이 쉽게 되지 않기 위해서 이런 캡차를 해놓은 것이랍니다. \n",
    "\n",
    "여담으로 캡차는 고서를 찍어서 올리는 경우도 있는데요, 이런 캡차도 있지만, 다른 캡차들 중에는 고서에서 컴퓨터가 알아보기 힘들지만, 사람은 입력할 수 있는 단어를 찍어서 올리기도 해요. 그래서 그 데이터를 쌓아서 충분한 데이터가 모이면 컴퓨터도 이제 그것을 읽을 수 있도록 학습하는 거죠. 신기하죵😉\n",
    "\n",
    "Anyway, 캡차를 피해가는 방법도 있지만, 이게 네이버에서 수시로 피해가는 방법을 막아버리기도 하고, 캡차를 넘어서면서까지 크롤링할 필요는 아직 없어서 다음 사이트에서 로그인을 해볼게요. 앞서 어떻게 사용하는지 알았으니, 다음 로그인 페이지는 아이디와 패스워드를 입력해 클릭하는 것까지 생각해보시고 그 다음 코드를 보면 더 좋을 것 같네요! \n",
    "\n",
    "- [다음 로그인 사이트](https://accounts.kakao.com/login?continue=https%3A%2F%2Flogins.daum.net%2Faccounts%2Fksso.do%3Frescue%3Dtrue%26url%3Dhttps%253A%252F%252Fwww.daum.net%252F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome('/Users/harampark/Documents/chromedriver')\n",
    "driver.get(\"https://accounts.kakao.com/login?continue=https%3A%2F%2Flogins.daum.net%2Faccounts%2Fksso.do%3Frescue%3Dtrue%26url%3Dhttps%253A%252F%252Fwww.daum.net%252F\")\n",
    "\n",
    "elem_login = driver.find_element_by_id(\"id_email_2\")\n",
    "elem_login.clear()\n",
    "elem_login.send_keys(\"************\")\n",
    "time.sleep(3)\n",
    "\n",
    "elem_login = driver.find_element_by_id(\"id_password_3\")\n",
    "elem_login.clear()\n",
    "elem_login.send_keys(\"************\")\n",
    "time.sleep(3)\n",
    "\n",
    "xpath = \"\"\"//*[@id=\"login-form\"]/fieldset/div[8]/button\"\"\"\n",
    "driver.find_element_by_xpath(xpath).click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/004.png)\n",
    "\n",
    "이렇게 나오면 성공! 그 다음에는 메일에 들어가 메일 사용자들을 크롤링해보려 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
